In [6]:

## Base Seq2Seq Model

Seq2Seq models are constructed using components like Long Short-Term Memory (LSTM) units; which are specialized types of recurrent neural networks (RNNs) that effectively capture temporal dependencies in data: LSTMs help in processing sequences by remembering important information over longer periods and forgetting irrelevant details, making them useful for handling tasks involving sequences of varying lengths. key

<!-- image -->

https://www.kaggle.com/code/harshjain123/machine-translation-seq2seq-lstms by Harsh Jain very insightful to understand this process; its will be a great place to understand the data processing pipeline

```
                                                                                                                                                                                                        | found the following notebook https://www.kaggle.com/code/harshjain123/machine-translation-seq2seq-lstms
                                                                                                                                                                                                       
                                                                                                                                                                                                       |
```